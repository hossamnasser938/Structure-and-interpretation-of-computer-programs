# 1.2 Procedures and the processes they generate

- The ability to visualize the effects of applying procedures is crucial to be an expert in programming. This visualization ability helps in designing and planning programs that exhibit the desired behaviour.

## 1.2.1 Liear recursion and iteration

- **calculating factorial**

1.

```
(define (factorial n)
        (if (= n 1)
            1
            (* n (factorial (- n 1)))))
```

2.

```
(define (factorial n)
        (factorial 1 1 n))

(define (factorial-iter product counter max-count)
        (if (> counter max-count)
            (product)
            (factorial-iter (* product counter) (+ counter 1) max-count)))
```

- Let's visualize the processes generated by these 2 distinct procedures:

1. The first implementation can be visualized:

```
(factorial 4)
(* 4 (factorial 3))
(* 4 3 (factorial 2))
(* 4 3 2 (factorial 1))
(* 4 3 2 1)
(* 4 3 2)
(* 4 6)
(* 24)
```

This process grows **maintaining deferred operations**(multiplications in this case) and then shrink by carrying out those operations later. The number of operations needed to be deferred is proportional to `n`. The interpreter must keep those deferred operations before starting to shrink. This type of processes is known as **linear recursive processes**.

2. The second implementation can be visualized:

```
(factorial 4)
(factorial-iter 1 1 4)
(factorial-iter 1 2 4)
(factorial-iter 2 3 4)
(factorial-iter 6 4 4)
(factorial-iter 24 5 4)
24
```

This process evolves **maintaining only 3 variables** whatever the value of `n` is. At any time, the state of the calculation is kept into the values of those 3 variables. This type of processes is known as **iterative processes**.

- Notice that while the 2 procedures are recursive, one of them produces a recursive process and the other produces an iterative process.

  - A **recursice procedure** is a procedure that is implemented in such a way that it **invokes itself as one of its teps**.
  - A **recusrsive process** is a process that **evolves maintaining deferred operations**.

- Notice that not all programming languages implements **iterative processes generated from recusrsive procedures** with the evaluation model described above. Languages such as **C and Pascal** evaluate recursive procedures in a way that the amount of memory needed is proportional to the number of procedure calls even if the process is iterative in principal. **Lisp**, in contrast, evaluates such procedures as we described making constructs like `for`, and `while` are syntactic sugar. **Lisp** uses **tail-recursive** to accomplish this.

### Exercise 1.9

```
(define (+ a b)
        (if (= a 0)
            b
            (inc (+ (dec a) b))))
```

```
(+ 4 5)
(inc (+ 3 5))
(inc (inc (+ 2 5)))
(inc (inc (inc (+ 1 5)))
(inc (inc (inc (inc (+ 0 5))))
(inc (inc (inc (inc 5))))
(inc (inc (inc 6)))
(inc (inc 7))
(inc 8)
9
```

**Recursive process**

```
(define (+ a b)
        (if (= a 0)
            b
            (+ (dec a) (inc b))))
```

```
(+ 4 5)
(+ 3 6)
(+ 2 7)
(+ 1 8)
(+ 0 9)
9
```

**Iterative process**

## 1.2.2 Tree recursion

- Fibonacci series:

```
fib(n) = 0                      if n = 0
         1                      if n = 1
         fib(n -1) + fib(n - 2) otherwise
```

- Calculating Fibonacci series:

```
(define (fib n)
        (cond ((= n 0) 0)
              ((= n 1) 1)
              (else (+ (fib (- n 1)) (fib (- n 2))))
        )
)
```

- This recursive procedure is an example of a tree recursion. If you visualized the process created by such a procedure, it will look exactly like a tree. The complexity of this procedure is:

  - **Exponential** in **time**.
  - **Linear** in **space** since the number of **stack frames** created by the procedure **does not exceed the maximum depth of the tree** and **the maximum depth is proportional to the input size**. The fact that "the number of stack frames created by the procedure does not exceed the maximum depth of the tree" is because calculating fib(n - 1) is always done before starting in calculating fib(n - 2) and hence stack frames **allocated** by calculating fib(n - 1) are **freed** before starting in calculating fib(n - 2). Look at this [stack overflow answer](https://stackoverflow.com/a/28756172/7034517).

- This procedure for calculating fibonacci series is **terrible** since it caries out significant number of **redundant calculations**. Such a defect can be optimized using **memoization**. Here is a **JavaScript** program that shows how **memoization** can enhance performance. I used **JavaScript** since I do not know how to do memoization using LISP.

```js
function naiveFibonacci(n) {
  if (n == 0) return 0;
  else if (n == 1) return 1;
  else return naiveFibonacci(n - 1) + naiveFibonacci(n - 2);
}

const hashTable = {};

function betterFibonacci(n) {
  if (n == 0) return 0;
  else if (n == 1) return 1;
  else {
    if (hashTable[n] != undefined) {
      return hashTable[n];
    }
    const fibOfN = betterFibonacci(n - 1) + betterFibonacci(n - 2);
    hashTable[n] = fibOfN;
    return fibOfN;
  }
}

const n = 50;
const calcSeconds = (ms) => ms / 1000;

console.log(`start calculating naiveFibonacci(${n})`);
const startNaive = Date.now();
console.log(naiveFibonacci(n));
const endNaive = Date.now();
console.log(
  `end calculating naiveFibonacci(${n}) in ${calcSeconds(
    endNaive - startNaive
  )} seconds`
);

console.log(`start calculating betterFibonacci(${n})`);
const startBetter = Date.now();
console.log(betterFibonacci(n));
const endBetter = Date.now();
console.log(
  `end calculating betterFibonacci(${n}) in ${calcSeconds(
    endBetter - startBetter
  )} seconds`
);
```

- Another way to calculate fibonacci series in an **iterative manner** is by **maintaining 2 variables representing fib(n - 1) and fib(n - 2)**:

```
(define (fib n)
        (fib-iter 1 0 n)
)

(define (fib-iter a b count)
        (cond ((= count 0) b)
        (else (fib-iter (+ a b) a (- count 1)))
        )
)
```

This procedure's complexity is **linear** in **time** and **constant** in **space**.

### Example: Counting change

```
counting_change(a, coins) = counting_change(a, coins_without_the_first) +
                            counting_change(a - d, coins)
```

```
(define (coin i) (
  cond ((= i 1) 1)
       ((= i 2) 5)
       ((= i 3) 10)
       ((= i 4) 25)
       ((= i 5) 50)
))

(define (count_change_iter a n) (
  cond ((= a 0) 1)
       ((< a 0) 0)
       ((= n 0) 0)
       (else (+ (count_change_iter a (- n 1)) (count_change_iter (- a (coin n)) n)))
))

(define (count_change a) (count_change_iter a 5))
```

- memoization is also called tabulation(refering to the use of hash table to avoid recomputation)

- We can also use memoization to get a better performance:

```js
const first = (arr) => arr[arr.length - 1];
const last = (arr) => arr[0];
const calcSeconds = (ms) => ms / 1000;
const removeFirst = (arr) => {
  const arrWithoutFirst = [...arr];
  arrWithoutFirst.pop();
  return arrWithoutFirst;
};

function countChange(amount, coins) {
  if (amount === 0) return 1;
  else if ((amount < 0) | (coins.length === 0)) return 0;
  else
    return (
      countChange(amount, removeFirst(coins)) +
      countChange(amount - first(coins), coins)
    );
}

const hashTable = {};
function memoizedCountChange(amount, coins) {
  if (amount === 0) return 1;
  else if ((amount < 0) | (coins.length === 0)) return 0;
  else {
    const hashTableKey = `${amount}_${first(coins)}_${last(coins)}`;
    if (hashTable[hashTableKey]) {
      return hashTable[hashTableKey];
    }
    const changeCount =
      memoizedCountChange(amount, removeFirst(coins)) +
      memoizedCountChange(amount - first(coins), coins);
    hashTable[hashTableKey] = changeCount;
    return changeCount;
  }
}

// problem input
const coinsArr = [25, 23, 20, 18, 15, 12, 10, 8, 5, 3, 1];
const amount = 200;

console.log("start memoized algorithm");
const beforeMemoized = Date.now();
const memoizedResult = memoizedCountChange(amount, coinsArr);
const afterMemoized = Date.now();
console.log(
  `memoized algorith end with result = ${memoizedResult} in ${calcSeconds(
    afterMemoized - beforeMemoized
  )} seconds`
);

console.log("start naive algorithm");
const beforeNaive = Date.now();
const naiveResult = countChange(amount, coinsArr);
const afterNaive = Date.now();
console.log(
  `naive algorith end with result = ${naiveResult} in ${calcSeconds(
    afterNaive - beforeNaive
  )} seconds`
);
```

- What's interesting here, is that the naive algorithm is better that the memoized one since calculation does not take significant time. If we changed the problem input to:

```js
const coinsArr = [10, 9, 8, 7, 6, 5, 4, 3, 2, 1];
const amount = 100;
```

We find that the memoized algorithm started to be better than the naive one. We can conclude that memoization is useful only when the calculation takes significant time and it is not the case all the time.

#### Exercise 1.11

```
f(n) = 3                                when n < 3
       f(n - 1) + 2f(n - 2) + 3f(n - 3) when n >= 3
```

- Recursive process:

```
(define (f n) (
  cond ((< n 3) 3)
       (else (+ (f (- n 1)) (* 2 (f (- n 2))) (* 3 (f (- n 3)))))
))
```

- Iterative process:
  let x refers to f(n - 1)
  and y refers to f(n - 2)
  and z refers to f(n - 3)

```
(define (f n) (f-iter n 2 3 3 3))

(define (f-iter n counter x y z) (
  cond ((< n 3) 3)
       ((> counter n) x)
       (else f-iter n (+ counter 1) (+ x (* 2 y) (* 3 z)) x y)
))
```

## 1.2.3 Orders of Growth

- **Orders of growth** is a **notion** we use to describe the **resources** consumed by a given process. It relates the **resources** consumed to the **problem input**. So it answers the question: "How much extra resources needed on increasing the problem input by a given factor?". We use the notion `R(n) = theta(f(n))` such that:

  - `R` is the resources consumed
  - `n` is the problem input
  - `f(n)` is a function such that `(k - 1)f(n) <= R(n) <= (k - 2)f(n)`

- If a process is described by: `R(n) = theta(n)` then this means that the resources consumed is increasing linearly with the problem input. Hence, doubling the problem input will double the resources consumed by the process.

- If a process is described by: `R(n) = theta(1)` then this means that the resoucres consumed in constant regardless the iproblem input. Hence, increasing the problem input has no effect on the resources consumed by the process.

- **Order of growth** does not accurately describe the cost of running a process but it highlights the cost of increasing the problem input for a given process. It is not accurate since we neglect considerable details to get a high-level estimation. For example, we treat the **multiplication** as one machine instruction regardless the size of numbers to be multiplicated. Hence, Process analysis should be done on various levels of abstractions exactly like program design.

## 1.2.4 Exponentiation

- Let's compute the exponential of a given number. Recall that `b^n = b * b^(n -1)`:

```
(define (exponent b n) (
  cond ((= n 0) 1)
       (else (* b (exponent b (- n 1))))
))
```

This is a **linear recursive** procedure.

- We can easily define a **linear iterative** procedure by maintaining an additional argument to the procedure that keeps the state of the product along with the procedure call:

```
(define (exponent b n)
  (exponent-iter b n 1))

(define (exponent-iter b counter product) (
  cond ((= counter 0) product)
       (else (exponent-iter b (- counter 1) (* product b)))
))
```

- We can go beyond the the fact that we need n stpes to calculate `b^n` so to calculate `b^8` intead of doing this:

```
result = b * (b * (b * (b * (b * (b * (b * (b)))))))
```

we can do this:

```
b^2 = b * b
b^4 = b^2 * b^2
b^8 = b^4 * b^4
```

in just 3 stpes rather than 8. This applies when the exponent is an even number. But we can make good use of this rule to define a general procedure that fits even and odd exponents:

```
b^n = (b^(n/2))^2 when n is even
b^n = b * b^(n-1) when n is odd
```

We can translate this rule into a porcedure:

```
(define (fast-exp b n)
  (cond ((= n 0) 1)
        ((even? n) (square (fast-exp b (/ n 2))))
        (else (* b (fast-exp b (- n 1))))
  )
)

(define (even? n)
  (= (remainder n 2) = 0)
)
```

This procedure has a **logarithmic** time and space complexity. So doubling the exponent(problem input) increases the steps ad space by one unit.

### Exercise 1.16

```
(define (fast-exp b n) (fast-exp-iter b n 1))

(define (fast-exp-iter b counter product)
  (cond ((= counter 0) (max product b))
        ((= counter 1) (max product b))
        ((even? counter) (fast-exp-iter b (/ counter 2) (square (max product b))))
        (else (fast-exp-iter b (- counter 1) (* product b)))
  )
)
```

### Exercise 1.17

- **Declarative** relationship

```
a * b = 2a * b/2          when b is even
a * b = a + a * (b - 1)   when b is odd
```

- **Imperative** implementation

```
(define (mul-fast a b)
  (cond ((= b 1) a)
        ((even? b) (mul-fast (double a) (halve b)))
        (else (+ a (mul-fast a (- b 1))))
  )
)
```

### Exercise 1.18

- **Declarative** relationship

```
a * b = 0 + 2a * b/2    when b is even
c` = 0,  a` = a2, b` = b/2
a * b = a + a  * b-1    when b is odd
c` = a,  a` = a, b` = b-1
```

- **Imperative** implementation

```
(define (mul-fast a b) (mul-fast-iter a b 0))

(define (mul-fast-iter a b c)
  (cond ((= b 0) c)
        ((even? b) (mul-fast-iter (double a) (halve b) c))
        (else (mul-fast-iter a (- b 1) (+ a c)))
)
```

## 1.2.5 Greatest Common Divisors

- Euclid's algorithm for computing the greatest common divisor:

```
(define (gcd a b) (
  if (= b 0)
     a
     (gcd b (remainder a b))
))
```

## 1.2.6 Example: Testing for Primality

1.  Finding the samllest integral divisor(greater than 1) of a given number.

```
(define (smallest-divisor n) (find-divisor n 2))

(define (find-divisor n test-divisor) (
  cond ((> (square test-divisor) n) n)
       ((= (remainder n test-divisor) 0) test-divisor)
       (else (find-divisor n (+ test-divisor 1)))
))
```

This algorithm is based on the mathematical fact that is if `n` is not a prime then there is a divisor for `n` less that `sqrt(n)`

- **Testing primality**

```
(define (prime? n) (= (smallest-divisor n) n))
```

This algorithm has to test the numbers between `2` and `sqrt(n)` hence it has **order of growth** `theta(sqrt(n))`

2. **The Fermat test**
   The following algorithm is based on **Fermat theorem** which states that if `n` is prime then for any positive numbers `a` less than `n`, `a^n % n = a`

```
(define (expomod base exp m) (remainder (fast-exp base exp) m))

(define (fermat-test n) (
  define (try-it a)
    (= (expomod a n n) a)
  (try-it (+ 1 (random (- n 1)))
))

(define (fast-prime? n times)(
  cond ((= times 0) true)
       ((fermat-test n) (fast-prime? n (- times 1)))
       (else false)
))
```

### Probabilistic methods

- The fermat test can make sure that a number is not prime. But it cannot make sure 100% that a number is prime. Because, there exists numbers that fooled the fermat test. But these numbers are rare and by increasing the test times, we can decrease the error rate as we wish.

### Exercise 1.21
